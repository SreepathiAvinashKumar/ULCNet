{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1323000])\n",
      "After STFT : torch.Size([1, 257, 5168])\n",
      "------ CRN Mag estimation block----------\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "After concatenated subbands torch.Size([8, 128, 40, 5168])\n",
      "after permute torch.Size([128, 40, 41344])\n",
      "After the gru : torch.Size([128, 40, 64])\n",
      "After the point wise conv torch.Size([128, 64, 40, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (40x64 and 32x257)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 134\u001b[0m\n\u001b[0;32m    131\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_waveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    137\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, clean_waveform)\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m, in \u001b[0;36mSpeechEnhancementModel.forward\u001b[1;34m(self, noisy_signal)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------ CRN Mag estimation block----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Stage 1: CRN-based Magnitude Estimation\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m magnitude_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmagnitude_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_mag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# print(\"Magnitude Mask\",magnitude_mask)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# print(\"Magnitude Mask shape\",magnitude_mask.shape)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---- CNN based phase enhancement--------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sripa\\Desktop\\Meeami\\Coding\\UltraLowSound\\CRN.py:140\u001b[0m, in \u001b[0;36mCRNBasedMagnitudeEstimation.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Fully Connected Layers for Intermediate Mask Estimation\u001b[39;00m\n\u001b[0;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# take the last time step output\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sripa\\envs\\meeami\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (40x64 and 32x257)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from CustomDataLoader import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from CRN import CRNBasedMagnitudeEstimation\n",
    "from CNNPhaseEnhance import CNNPhaseEnhancement\n",
    "from CleanSpeechEstimate import CleanSpeechEstimation\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# There we get Channel Dimension only if we get the complex numbers as individual values, otheriwse if we get as complex_value = True then all are as 1 on channel dimension\n",
    "\n",
    "class SpeechEnhancementModel(nn.Module):\n",
    "    def __init__(self, crn_params, cnn_params, power_law=0.3):\n",
    "        super(SpeechEnhancementModel, self).__init__()\n",
    "        self.power_law = power_law\n",
    "        self.magnitude_estimation = CRNBasedMagnitudeEstimation(**crn_params)\n",
    "        self.phase_enhancement = CNNPhaseEnhancement(**cnn_params)\n",
    "        self.clean_speech_estimation = CleanSpeechEstimation(power_law)\n",
    "\n",
    "    def stft(self,x, fft_size=512, hop_size=256, window='hann'): \n",
    "    # Create window function\n",
    "        if window == 'hann':\n",
    "            window = torch.hann_window(fft_size)\n",
    "        else:\n",
    "            raise ValueError(\"only hann window is supported.\")\n",
    "\n",
    "        # Compute STFT\n",
    "        stft_matrix = torch.stft(x, fft_size, hop_size, window=window,return_complex=True) #  = false\n",
    "        return stft_matrix\n",
    "\n",
    "\n",
    "    def power_law_compression(self, signal):\n",
    "        real = signal.real \n",
    "        imag = signal.imag \n",
    "\n",
    "        compressed_real = torch.pow(torch.abs(real), self.power_law) * torch.sign(real)\n",
    "\n",
    "        compressed_img = torch.pow(torch.abs(imag), self.power_law) * torch.sign(imag)\n",
    "\n",
    "        compressed_mag = torch.sqrt(torch.square(compressed_real)+torch.square(compressed_img))\n",
    "        compressed_phase = torch.arctan(compressed_img/compressed_real)\n",
    "\n",
    "        return [compressed_mag,compressed_phase]\n",
    "    \n",
    "    def inv_stft(self,complex_tensor,n_fft=512,hop_length=16,win_length=32):\n",
    "        time_domain_signal = torch.istft(complex_tensor, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "        return time_domain_signal\n",
    "\n",
    "    def forward(self, noisy_signal):\n",
    "        print(noisy_signal.shape)\n",
    "        # Apply stft \n",
    "        stft_matrix = self.stft(noisy_signal)\n",
    "\n",
    "        print(\"After STFT :\",stft_matrix.shape)\n",
    "\n",
    "        # Apply power law compression\n",
    "        compressed_mag,compressed_phase = self.power_law_compression(stft_matrix)\n",
    "\n",
    "        print(\"------ CRN Mag estimation block----------\")\n",
    "        # Stage 1: CRN-based Magnitude Estimation\n",
    "        magnitude_mask = self.magnitude_estimation(compressed_mag)\n",
    "\n",
    "        # print(\"Magnitude Mask\",magnitude_mask)\n",
    "        # print(\"Magnitude Mask shape\",magnitude_mask.shape)\n",
    "\n",
    "\n",
    "        print(\"---- CNN based phase enhancement--------\")\n",
    "        # Stage 2: CNN-based Phase Enhancement\n",
    "        complex_mask = self.phase_enhancement(magnitude_mask, compressed_phase)\n",
    "        \n",
    "        # Clean Speech Estimation\n",
    "        estimated_clean_speech = self.clean_speech_estimation(complex_mask, noisy_signal)\n",
    "        \n",
    "        return estimated_clean_speech\n",
    "\n",
    "# Example usage\n",
    "crn_params = {\n",
    "    'in_channels': 1, # 64 \n",
    "    'out_channels': 128, \n",
    "    'hidden_size': 32, \n",
    "    'num_subbands': 8\n",
    "}\n",
    "cnn_params = {\n",
    "    'in_channels': 2, \n",
    "    'out_channels': 64\n",
    "}\n",
    "\n",
    "\n",
    "csv_file = './DataLoad/files.csv'\n",
    "train = './DataLoad/train'\n",
    "label = './DataLoad/label'  \n",
    "batches = 1\n",
    "\n",
    "mdataset = MyDataset(csv=csv_file,train_dir=train,label_dir=label)\n",
    "\n",
    "train_set ,test_set  = torch.utils.data.random_split(mdataset,[23,23])\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size=batches,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=batches,shuffle=True)\n",
    "\n",
    "\n",
    "sample_rate = 16000\n",
    "duration = 10\n",
    "channels = 2\n",
    "\n",
    "num_epochs = 1\n",
    "learning_rate = 0.0004\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SpeechEnhancementModel(crn_params, cnn_params)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=10)\n",
    "\n",
    "# noisy_signal = torch.randn(10, 64, 256, 100)  # (batch_size, channels, freq_bins, time_steps)\n",
    "# print(output.shape)  # Expected output shape: (batch_size, freq_bins, time_steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        noisy_waveform, clean_waveform = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(noisy_waveform)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, clean_waveform)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "                    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "\n",
    "# summary(model,torch.tensor(mdataset.__getitem__(0)[0]))\n",
    "# k = model(torch.tensor(mdataset.__getitem__(0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "info = torchaudio.info(mdataset.give_path(0))\n",
    "\n",
    "num_channels = info.num_channels\n",
    "\n",
    "print(num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00372978 -0.00413349 -0.00312093 ...  0.08128223  0.09374231\n",
      "  0.06016465]\n",
      "(1323000,)\n"
     ]
    }
   ],
   "source": [
    "k = mdataset.__getitem__(0)\n",
    "print(k[0])\n",
    "print(k[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(DepthwiseSeparableConv2D, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,stride=stride,padding=padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,downsample=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = DepthwiseSeparableConv2D(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        if self.downsample:\n",
    "            self.pool(x)\n",
    "        return x\n",
    "\n",
    "class FGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True):\n",
    "        super(FGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        return x\n",
    "\n",
    "class TemporalGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(TemporalGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        return x\n",
    "\n",
    "class CRNBasedMagnitudeEstimation(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_size, num_subbands, kernel_size=(1, 3), stride=1, padding=1):\n",
    "        super(CRNBasedMagnitudeEstimation, self).__init__()\n",
    "        self.num_subbands = num_subbands\n",
    "        \n",
    "        # Convolutional Block with Channelwise Feature Reorientation\n",
    "\n",
    "        # filters used each conv respectively\n",
    "        filters = [32,64,96,128]\n",
    "\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            ConvBlock(in_channels // num_subbands, filters[i], kernel_size, stride, padding,downsample=(i>0))\n",
    "            for i in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "        # Frequency-axis Bidirectional GRU\n",
    "        self.fgru = FGRU(filters[1], hidden_size)\n",
    "\n",
    "        # Point-wise Convolution\n",
    "        self.pointwise_conv = nn.Conv2d(filters[1], hidden_size, kernel_size=1)\n",
    "\n",
    "        # Temporal GRU\n",
    "        self.temporal_gru = TemporalGRU(hidden_size, hidden_size)\n",
    "\n",
    "        # Fully Connected Layers for Intermediate Mask Estimation\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channels, freq_bins, time_steps = x.size()\n",
    "        \n",
    "        # Channelwise Feature Reorientation\n",
    "        subband_outputs = []\n",
    "        subband_size = freq_bins // self.num_subbands\n",
    "        for i in range(self.num_subbands):\n",
    "            start = i * subband_size\n",
    "            end = start + subband_size\n",
    "            subband = x[:, start:end, :]\n",
    "            subband_output = self.conv_blocks[i](subband.unsqueeze(0))\n",
    "            subband_outputs.append(subband_output.squeeze(0))\n",
    "        \n",
    "        # Concatenate subband outputs\n",
    "        x = torch.cat(subband_outputs, dim=1)\n",
    "        \n",
    "        # Frequency-axis Bidirectional GRU\n",
    "        x = x.permute(1,0,2).contiguous().view(freq_bins, -1)  # (freq_bins, channels * time_steps)\n",
    "\n",
    "        x = self.fgru(x.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Point-wise Convolution\n",
    "        x = x.permute(1,0).view(-1, freq_bins, time_steps).unsqueeze(0) # (batch_size, hidden_size, freq_bins, time_steps)\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        # Temporal GRU\n",
    "        x = x.permute(3, 0, 1, 2).contiguous().view(time_steps, -1)  # (time_steps, batch_size, hidden_size * freq_bins)\n",
    "        x = self.temporal_gru(x.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Fully Connected Layers for Intermediate Mask Estimation\n",
    "        x = x[-1]  # take the last time step output\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNPhaseEnhancement(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(1, 3), stride=1, padding=1):\n",
    "        super(CNNPhaseEnhancement, self).__init__()\n",
    "        \n",
    "        # Intermediate Feature Computation (Noisy Phase and Intermediate Real Magnitude Mask)\n",
    "        self.combine_features = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # CNN Processing\n",
    "        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "        # Complex Mask Estimation\n",
    "        self.complex_mask_estimation = nn.Conv2d(out_channels, 2, kernel_size=1)  # 2 for real and imaginary parts # Pointwise conv\n",
    "\n",
    "    def forward(self, magnitude_mask, noisy_phase):\n",
    "\n",
    "        x = torch.cat((magnitude_mask, noisy_phase), dim=1)\n",
    "        x = self.combine_features(x)   # Combine magnitude mask and noisy phase = Internmediate feature computation \n",
    "\n",
    "        \n",
    "        # CNN Processing\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Complex Mask Estimation\n",
    "        complex_mask = self.complex_mask_estimation(x)\n",
    "        \n",
    "        return complex_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanSpeechEstimation(nn.Module):\n",
    "    def __init__(self, power_law=0.3):\n",
    "        super(CleanSpeechEstimation, self).__init__()\n",
    "        self.power_law = power_law\n",
    "\n",
    "    def forward(self, complex_mask, noisy_signal):\n",
    "        # Estimated clean speech signal\n",
    "        real_mask = complex_mask[:, 0, :, :]\n",
    "        imag_mask = complex_mask[:, 1, :, :]\n",
    "        estimated_complex_speech = (real_mask + 1j * imag_mask) * noisy_signal\n",
    "        \n",
    "        # Power law decompression\n",
    "        estimated_clean_speech = torch.pow(torch.abs(estimated_complex_speech), 1/self.power_law) * torch.sign(estimated_complex_speech)\n",
    "        \n",
    "        return estimated_clean_speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meeami",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
